
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script type="text/javascript" charset="utf-8" src="https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script> 
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
}

@media screen and (min-width: 980px){
    body {
        width: 980px;
    }
}

h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #B6486F;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1, h2 {
    text-align: center;
}
h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
h3 {
    display: inline;
}
.paper-title {
    padding: 16px 0px 16px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.col-1 {
     width: 100%;
     float: left;
}

.author-row, .affil-row {
    font-size: 26px;
}

.author-row-new { 
    text-align: center; 
}

.author-row-new a {
    display: inline-block;
    font-size: 26px;
    padding: 15px;
}

.author-row-new sup {
    color: #313436;
    font-size: 60%;
}

.affiliations-new {
    font-size: 18px;
    text-align: center;
    width: 80%;
    margin: 0 auto;
    margin-bottom: 20px;
}

.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 26px;
}
.affiliatons {
    font-size: 18px;
}
.affil-row {
    margin-top: 18px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.inline-title h3 {
      display: inline;
    }
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    color: #666;
    margin-top: 4px;
    margin-bottom: 10px;
}
video {
    display: block;
    margin: auto;
}
figure {
    display: inline-block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
figure figcaption {
    text-align: center;
    font-size: 14px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    padding: 0;
    margin: 0;
    list-style: none;
}

.wrapper {
    display: flex;
}

.paper-btn-coming-soon {
    position: relative; 
    top: 0;
    left: 0;
}

.coming-soon {
    position: absolute;
    top: -15px;
    right: -15px;
}

.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #B6486F;
  color: white !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}
.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}

.paper-btn:hover {
    opacity: 0.85;
}

.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}

.venue {
    font-size: 30px;
}

.topnav {
    background-color: #EEEEEE;
    overflow: hidden;
}

.topnav div {
    max-width: 1070px;
    margin: 0 auto;
}

.topnav a {
    display: inline-block;
    color: black;
    text-align: center;
    vertical-align: middle;
    padding: 16px 16px;
    text-decoration: none;
    font-size: 18px;
}

.topnav img {
    padding: 2px 0px;
    width: 100%;
    margin: 0.2em 0px 0.3em 0px;
    vertical-align: middle;
}

pre {
    font-size: 0.9em;
    padding-left: 7px;
    padding-right: 7px;
    padding-top: 3px;
    padding-bottom: 3px;
    border-radius: 3px;
    background-color: rgb(235, 235, 235);
    overflow-x: auto;
}

.download-thumb {
    display: flex;
}

@media only screen and (max-width: 620px) {
    .download-thumb {
        display: none;
    }
}

.paper-stuff {
    width: 50%;
    font-size: 20px;
}

@media only screen and (max-width: 620px) {
    .paper-stuff {
        width: 100%;
    }
}

</style>

<script type="text/javascript" src="../js/hidebib.js"></script>
    <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
    <head>
        <title>Differentially Private Diffusion Models</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:description" content="Differentially Private Diffusion Models"/>
        <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:creator" content="@timudk">
        <meta name="twitter:title" content="Differentially Private Diffusion Models">
        <meta name="twitter:description" content="Add.">
        <meta name="twitter:image" content="https://nv-tlabs.github.io/DPDM/assets/pipeline_resized.png">
    </head>

 <body>
<div class="topnav" id="myTopnav">
    <div>
        <a href="https://www.nvidia.com/"><img width="100%" src="assets/nvidia.svg"></a>
        <a href="https://nv-tlabs.github.io/" ><strong>Toronto AI Lab</strong></a>
    </div>
</div>
<div class="container">
    <div class="paper-title">
      <h1>Differentially Private Diffusion Models</h1> 
    </div>

    <div id="authors">
    	<center>
            <div class="author-row-new">
                <a href="https://timudk.github.io/">Tim Dockhorn<sup>1,2,3</sup></a>
                <a href="https://orcid.org/0000-0001-6579-6044">Tianshi Cao<sup>1,3,4</sup></a>
                <a href="http://latentspace.cc/">Arash Vahdat<sup>1</sup></a>
                <a href="https://karstenkreis.github.io/">Karsten Kreis<sup>1</sup></a>
            </div>
        </center>
        <center>
        <div class="affiliations">
            <span><sup>1</sup> NVIDIA</span>
            <span><sup>2</sup> University of Waterloo</span>
            <span><sup>3</sup> Vector Institute</span>
            <span><sup>4</sup> University of Toronto</span> <br/>
        </div>

        </center>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="paper-btn" href="https://arxiv.org/abs/2210.09929">
                <span class="material-icons"> description </span> 
                 Paper
            </a>
            <div class="paper-btn-coming-soon">
                <a class="paper-btn" href="https://github.com/nv-tlabs/DPDM">
                    <span class="material-icons"> code </span>
                    Code
                </a>
            </div>
        </div></div>
    </div>
    <br>

    <section id="abstract"/>
        <h2>Abstract</h2>
        <hr>
        <div class="flex-row">
            <p>
                While modern machine learning models rely on increasingly large training datasets, data is often limited in privacy-sensitive domains. 
                Generative models trained with differential privacy (DP) on sensitive data can sidestep this challenge, providing access to synthetic 
                data instead. However, training DP generative models is highly challenging due to the noise injected into training to enforce DP. We 
                propose to leverage diffusion models (DMs), an emerging class of deep generative models, and introduce <em>Differentially Private Diffusion 
                Models</em> (DPDMs), which enforce privacy using differentially private stochastic gradient descent (DP-SGD). We motivate why DP-SGD is well 
                suited for training DPDMs, and thoroughly investigate the DM parameterization and the sampling algorithm, which turn out to be crucial ingredients 
                in DPDMs. Furthermore, we propose <em>noise multiplicity</em>, a simple yet powerful modification of the DM training objective tailored to the DP 
                setting to boost performance. We validate our novel DPDMs on widely-used image generation benchmarks and achieve state-of-the-art (SOTA) performance 
                by large margins. For example, on MNIST we improve the SOTA FID from 48.4 to 5.01 and downstream classification accuracy from 83.2% to 98.1% for 
                the privacy setting DP-\((\varepsilon{=}10, \delta{=}10^{-5})\). Moreover, on standard benchmarks, classifiers trained on DPDM-generated synthetic data 
                perform on par with task-specific DP-SGD-trained classifiers, which has not been demonstrated before for DP generative models.
            </p>
        </div>
    </section>

    <section id="teaser-image">
            </p>
            <figure style="margin-top: 20px; margin-bottom: 20px;">
                <center>
                <img width="80%" src="./assets/pipeline.png" style="margin-bottom: 20px;">
            </center>
                <p class="caption">
                    Information flow during training in our <em>Differentially Private Diffusion Models</em> (DPDMs) for a single 
                    training sample in <span style="color: #76B900">green</span> (i.e. batchsize \(B{=}1\), another sample shown in <span style="color: #03a9fc">blue</span>). We rely on DP-SGD to guarantee
                    privacy and use <em>noise multiplicity</em>. The diffusion is visualized for a one-dimensional toy distribution (marginal 
                    probabilities in <span style="color: #A03F77">purple</span>); our main experiments use high-dimensional images.
                </p><p class="caption">
            </p>
    </section>

    <section id="news">
        <h2>News</h2>
        <hr>
        <div class="row">
            <div><span class="material-icons"> event </span> [Oct 2022] <a href=https://twitter.com/timudk/status/1582717593718181891>Twitter thread</a> explaining the work in detail.</div> 
            <div><span class="material-icons"> event </span> [Oct 2022] <a href="https://nv-tlabs.github.io/DPDM">Project page</a> released!</div>
            <div><span class="material-icons"> event </span> [Oct 2022] Draft released on <a href="https://arxiv.org/abs/2210.09929">arXiv</a>!</div>
        </div>
    </section>


    <section id="intro"/>
        <h2>Differentially Private Diffusion Models</h2>
        <hr>
        <div class="flex-row">
            <p>Modern deep learning usually requires significant amounts of training data. However, sourcing 
            large datasets in privacy-sensitive domains is often difficult. To circumvent this challenge, generative 
            models trained on sensitive data can provide access to large synthetic data instead, which can be
            used flexibly to train downstream models. Unfortunately, typical overparameterized neural networks
            have been shown to provide little to no privacy to the data they have been trained on. For example,
            an adversary may be able to recover training images of deep classifiers using gradients of the
            networks. Generative models may even overfit directly, 
            generating data indistinguishable from the
            data they have been trained on. In fact, overfitting and privacy-leakage of generative models are more
            relevant than ever, considering recent works that train powerful photo-realistic image generators
            on large-scale Internet-scraped data. Since the latest variants of these impressive image generation systems
            leverage diffusion models, advancing specifically diffusion model-based generative modeling with privacy
            guarantees is a pressing topic.</p>
            
            <p>In this work, we propose Differentially Private Diffusion Models (DPDMs), diffusion models (DMs)
            trained with rigorous DP guarantees based on differentially private stochastic gradient descent (DP-SGD). Privacy in DP-SGD is enforced by clipping and noising parameter gradients. 
            We motivate why DMs are uniquely well suited for DP generative modeling, and we study DPDM parameterization, training setting and model sampling in detail, 
            and optimize it for the DP setup. We propose <em>noise multiplicity</em> to efficiently boost DPDM performance (see Figure above).
            Experimentally, we significantly surpass the state-of-the-art in DP synthesis on widely-studied image modeling benchmarks and we demonstrate that classifiers trained 
            on DPDM-generated data perform on par with task-specific DP-trained discriminative models. 
            This implies a very high utility of the synthetic data generated by DPDMs, delivering on the promise of DP generative models as an effective data sharing medium.</p>
        </div>
    </section>

    <section id="insights"/>
        <h2>Key Insights</h2>
        <hr>
            <figure style="margin-top: 20px;">
                <center>
                <img width="50%" src="./assets/complexity.png">
            </center>
                <p class="caption">
                    Frobenius norm of the Jacobian \(\mathcal{J}_F(\sigma)\) (measure of complexity) of the denoiser \(D\) at noise level \(\sigma\) and constant 
                    Frobenius norms of the Jacobians \(\mathcal{J}_F\) of the end-to-end sampling function defined by the DM as well as the one-shot synthesis network of a GAN. 
                    Less complex functions require smaller neural networks, which is beneficial for DP generative modeling; see paragraph below.
                </p><p class="caption">
            <p> <b>Sequential denoising.</b> The sampling function in DMs is defined through a sequential denoising process, 
                breaking the difficult generation task into many small denoising steps. We found that the denoiser \(D\), the learnable component in DMs,
                is significantly less complex than the one-shot synthesis neural network learned by a GAN or the end-to-end multi-step synthesis process of the DM (see Figure above).
                Generally, more complex functions require larger neural networks and are more difficult to learn. Importantly, the 
                magnitude of the noise added in the DP-SGD updates scales linearly with the number of parameters, and therefore smaller 
                networks are generally preferred. Consequently, DMs are well-suited for DP generative modeling with DP-SGD, considering the lower neural network complexity required in the denoiser.
            </p>
            <p> <b>Objective function.</b>  GANs are currently the predimomant class of generative models explored in DP generative modeling, despite them being difficult to optimize and prone to mode collapse due to their adversarial training scheme.
                This can be particularly problematic during noisy DP-SGD training. 
                In contrast, DMs are trained with a simple regression-like \(L_2\)-loss which makes them robust and scalable in practice, and therefore arguably better-suited for DP-SGD-based training.
            </p>
            <p> <b>Stochastic diffusion model sampling.</b> Generating samples from DMs
                with stochastic sampling can perform better than deterministic sampling when the score model is
                not learned well. Since we replace gradient estimates in DP-SGD training with biased large variance
                estimators, we cannot expect a perfectly accurate score model. We empirically show that
                stochastic sampling can in fact boost perceptual synthesis quality in DPDMs as measured by Fr&eacutechet Inception Distance (FID).
            </p>
            <p> <b>Noise multiplicity.</b>  When training DMs with DP-SGD, we incur a privacy
                cost for each iteration, and therefore prefer a smaller number of iterations than in the non-DP setting. Furthermore, since the
                per-example gradient clipping as well as the noise injection induce additional variance in DP-SGD, we would
                like our objective function to be less noisy than in the non-DP case. We achieve this through <emph>noise multiplicity</emph>,
                evaluating each data point for many noise levels \(\sigma\). 
                Importantly, we show that this modification comes at no additional privacy cost.
                The noise multiplicity mechanism is also highlighted in the pipeline figure above.
            </p>
            <p> <b>Noise level sampling during training.</b> Especially for high privacy settings (small DP-\(\varepsilon\)), we found it important
                to focus training of the denoiser, the learnable component in DMs, on high noise levels \(\sigma\).
                It is known that at large \(\sigma\) the DM learns the global coarse structure
                of the data, which is crucial to form visually coherent images that can also be used to train downstream
                models. This is relatively easy to achieve in the non-DP setting, due to the heavily smoothed diffused
                distribution at these high noise levels \(\sigma\). At high privacy levels, however, even training at such high
                noise levels \(\sigma\) can be challenging due to DP-SGD's gradient clipping and noising.
            </p>
        </div>
    </section>


    <section id="results">
        <h2>Experimental Results</h2>
        <hr>
        <div class="flex-row">
            <p>We extensively validate our DPDMs on several popular DP benchmark datasets, namely, (conditional) MNIST, (conditional) Fashion-MNIST, and (unconditional) CelebA (downsampled to 32x32 resolution). 
                We measure sample quality via FID. On MNIST and Fashion-MNIST, we also assess utility of class-labeled generated data by training classifiers on synthesized samples 
                and compute class prediction accuracy on real data. As is standard practice, we consider logistic regression (Log Reg), MLP, and CNN classifiers; see paper for details.
            </p>
        </div>
        <h2>MNIST & Fashion-MNIST</h2>
        <div class="wrapper">
            <center>
            <figure>
            <figcaption>Other methods in the literature</figcaption>
            <img width="70%" src="./assets/mnist_and_fashion_mnist.png" style="margin-bottom: 0px;">
            <figcaption>Our DPDM</figcaption>
        </div> 
        <p class="caption">
            (<em>Above</em>) MNIST and Fashion-MNIST images generated by existing methods (above black line) and our DPDM (below black line). The DP privacy setting is \((\varepsilon{=}10, \delta{=}10^{-5})\).
            (<em>Below</em>) Class-conditional DP image generation performance (MNIST & Fashion-MNIST) measured in FID and downstream classifier utility (\(\delta{=}10^{-5}\) and three \(\varepsilon\)).
            DP-MEPF (<span>&#8224;</span>) uses additional public data for training (only included for completeness).
        </p><p class="caption">
            <div class="wrapper">
                <center>
                <figure>
                <img width="70%" src="./assets/screenshot_table_one.png" style="margin-bottom: 0px;">
            </div> 
        <h2>CelebA</h2>
        <div class="wrapper">
            <center>
            <figure>
            <figcaption>Other methods in the literature</figcaption>
            <img width="50%" src="./assets/celeba.png" style="margin-bottom: 0px;">
            <figcaption>Our DPDM</figcaption>
        </div> 
        <p class="caption">
            (<em>Above</em>) CelebA images generated by existing methods (above black line) and our DPDM (below black line). The DP privacy setting is \((\varepsilon{=}10, \delta{=}10^{-6})\). 
            (<em>Below</em>) DP image generation performance on CelebA measured in FID (\(\delta{=}10^{-6}\) and two \(\varepsilon\)). 
            G-PATE and DataLens (<span>&#8224;</span>) use \(\delta{=}10^{-5}\) (less privacy) and model images at 64x64 resolution.
        </p><p class="caption">
            <center>
            <figure>
            <img width="45%" src="./assets/screenshot_table_four.png" style="margin-bottom: 0px;">
    </section>
    
    <section id="paper">
        <h2>Paper</h2>
        <hr>
        <div class="flex-row">
            <div class="download-thumb">
            <div style="box-sizing: border-box; padding: 16px; margin: auto;">
                <a href="https://nv-tlabs.github.io/DPDM"><img class="screenshot" src="assets/dpdm_paper_snapshot.png"></a>
            </div>
        </div>
            <div class="paper-stuff">
                <p><b>Differentially Private Diffusion Models</b></p>
                <p>Tim Dockhorn, Tianshi Cao, Arash Vahdat, Karsten Kreis</p>
                <div><span class="material-icons"> description </span><a href="https://arxiv.org/abs/2210.09929"> arXiv version</a></div>
                <div><span class="material-icons"> insert_comment </span><a href="assets/dockhorn2022differentially.bib"> BibTeX</a></div>
                <div><span class="material-icons"> integration_instructions </span><a href="https://github.com/nv-tlabs/DPDM"> Code</a></div>
            </div>
            </div>
        </div>
    </section>

    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>@article{dockhorn2022differentially,
    title={{Differentially Private Diffusion Models}},
    author={Dockhorn, Tim and Cao, Tianshi and Vahdat, Arash and Kreis, Karsten},
    journal={arXiv:2210.09929},
    year={2022}
}</code></pre>
    </section>
</div>
</body>
</html>